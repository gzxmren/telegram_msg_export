import csv
import os
import re
import requests
from collections import defaultdict

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.9',
}

def extract_url(text):
    match = re.search(r'https?://[^\s,]+', text)
    return match.group(0) if match else None

def get_url_category(url):
    parsed = re.search(r'https?://([^/]+)(/[^?#]*)', url)
    if not parsed: return "unknown"
    domain, path = parsed.groups()
    if 'weixin.qq.com' in domain:
        return "wechat_short" if '/s/' in path else "wechat_long"
    if 'x.com' in domain or 'twitter.com' in domain:
        return "x_status" if '/status/' in path else "x_video"
    return domain

def verify_link_verbose(url, clean_url):
    try:
        # ä½¿ç”¨ Session ä¿æŒä¸€å®šçš„æŒä¹…æ€§
        session = requests.Session()
        r1 = session.get(url, headers=HEADERS, timeout=10, allow_redirects=True)
        r2 = session.get(clean_url, headers=HEADERS, timeout=10, allow_redirects=True)
        
        len_orig, len_clean = len(r1.content), len(r2.content)
        len_ratio = abs(len_orig - len_clean) / (len_orig + 1)
        
        is_safe = (r1.status_code == r2.status_code) and (len_ratio < 0.1)
        
        print(f"    - [åŸå§‹] çŠ¶æ€: {r1.status_code}, é•¿åº¦: {len_orig}, æœ€ç»ˆåœ°å€: {r1.url[:50]}...")
        print(f"    - [æ¸…æ´—] çŠ¶æ€: {r2.status_code}, é•¿åº¦: {len_clean}, æœ€ç»ˆåœ°å€: {r2.url[:50]}...")
        
        # æ·±åº¦æ€€ç–‘ï¼šå¦‚æœä¸¤ä¸ªéƒ½è¢«é‡å®šå‘åˆ°äº† loginï¼Œé•¿åº¦æ²¡å‡†ä¹Ÿä¸€æ ·ï¼Œè¿™é‡Œè¦å°å¿ƒ
        if "login" in r1.url.lower() and "login" in r2.url.lower():
            print("    âš ï¸ è­¦å‘Šï¼šä¸¤ä¸ªé“¾æ¥éƒ½è¢«é‡å®šå‘åˆ°äº†ç™»å½•é¡µï¼Œå®éªŒæ•°æ®å¯èƒ½ä¸å¯ä¿¡ï¼ˆè¢«åçˆ¬è™«æ‹¦æˆªï¼‰ã€‚")
        
        return is_safe
    except Exception as e:
        print(f"    âŒ å®éªŒå¼‚å¸¸: {e}")
        return False

def run_deep_verify(csv_path):
    if not os.path.exists(csv_path): return
    categories = defaultdict(list)
    with open(csv_path, 'r', encoding='utf-8-sig') as f:
        for row in csv.DictReader(f):
            url = extract_url(row.get('content', ''))
            if not url: continue
            cat = get_url_category(url)
            if len(categories[cat]) < 2: categories[cat].append(url)

    print(f"\n===== æ·±åº¦éªŒè¯: {csv_path} =====")
    for cat, urls in categories.items():
        print(f"\n[ç±»åˆ«: {cat}]")
        for url in urls:
            cleaned = url.split('?')[0]
            if url == cleaned: continue
            print(f"  ğŸ” æµ‹è¯•é“¾æ¥: {url[:70]}...")
            safe = verify_link_verbose(url, cleaned)
            print(f"  ğŸ’¡ ç»“è®º: {'âœ… å®‰å…¨' if safe else 'âŒ é£é™©'}")

if __name__ == '__main__':
    run_deep_verify('data/x/x_url.csv')
    run_deep_verify('data/wechat/wechat_url.csv')
