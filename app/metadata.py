import asyncio
import aiohttp
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from app.logger import logger
from app.config import AppConfig
from typing import Optional
import re

class MetadataProvider:
    """ç½‘é¡µå…ƒæ•°æ®æŠ“å–å™¨ - é‡ç‚¹å…³æ³¨ç¨³å®šæ€§å’Œæ•ˆç‡"""
    
    def __init__(self):
        self.cache = {} # ç®€å•çš„å†…å­˜ç¼“å­˜: {url: title}
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        }

    def _get_proxy_url(self) -> Optional[str]:
        """ä» AppConfig æ„é€ ä»£ç† URL"""
        if AppConfig.PROXY_HOST and AppConfig.PROXY_PORT:
            # aiohttp åŸç”Ÿä»…æ”¯æŒ HTTP ä»£ç†
            if AppConfig.PROXY_TYPE.upper() == "HTTP":
                auth = ""
                if AppConfig.PROXY_USER and AppConfig.PROXY_PASS:
                    auth = f"{AppConfig.PROXY_USER}:{AppConfig.PROXY_PASS}@"
                return f"http://{auth}{AppConfig.PROXY_HOST}:{AppConfig.PROXY_PORT}"
        return None

    def _is_safe_url(self, url: str) -> bool:
        # ... (ä¿æŒåŸæœ‰ä»£ç ä¸å˜)
        try:
            parsed = urlparse(url)
            hostname = parsed.hostname.lower() if parsed.hostname else ""
            if not hostname: return False
            if hostname in ["localhost", "127.0.0.1", "0.0.0.0", "::1"]: return False
            if re.match(r'^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)', hostname):
                return False
            return True
        except:
            return False

    async def fetch_metadata(self, url: str) -> tuple[Optional[str], Optional[str]]:
        """æŠ“å–ç½‘é¡µå…ƒæ•°æ® (æ ‡é¢˜ + æœ€ç»ˆ URL)ï¼Œå¸¦ç¼“å­˜å’Œè‡ªåŠ¨ä»£ç†å›é€€æœºåˆ¶"""
        if not url: return None, None
        if not self._is_safe_url(url):
            logger.warning(f"ğŸ›¡ï¸ æ‹¦æˆªæ½œåœ¨çš„ SSRF è¯·æ±‚: {url}")
            return None, None
            
        if url in self.cache: return self.cache[url]

        domestic_patterns = [
            r"douyin\.com", r"iesdouyin\.com", r"weixin\.qq\.com", 
            r"zhihu\.com", r"bilibili\.com", r"weibo\.com", r"qq\.com"
        ]
        is_domestic = any(re.search(p, url) for p in domestic_patterns)
        
        async def try_fetch(use_proxy: bool):
            request_headers = self.headers.copy()
            proxy_url = self._get_proxy_url() if use_proxy else None
            
            if "douyin.com" in url or "iesdouyin.com" in url:
                request_headers.update({
                    "User-Agent": "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                })
            elif "mp.weixin.qq.com" in url:
                request_headers.update({
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
                    "Referer": "https://mp.weixin.qq.com/",
                })

            timeout = aiohttp.ClientTimeout(total=15, connect=10)
            async with aiohttp.ClientSession(
                headers=request_headers,
                max_line_size=16384,
                max_field_size=16384,
                trust_env=True # å§‹ç»ˆå…è®¸è¯»å–ç³»ç»Ÿç¯å¢ƒå˜é‡ä»£ç†
            ) as session:
                # å¦‚æœ use_proxy ä¸º True ä¸”é…ç½®äº†æ˜¾å¼ä»£ç†ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨
                async with session.get(url, timeout=timeout, allow_redirects=True, proxy=proxy_url) as response:
                    final_url = str(response.url)
                    if response.status != 200:
                        return None, final_url
                    
                    try:
                        # å¯¹äºå·¨å¤§çš„é¡µé¢ï¼ˆå¦‚å¾®ä¿¡ï¼‰ï¼Œæˆ‘ä»¬åªè¯»å–å‰ 256KB åŸå§‹æ•°æ®
                        # aiohttp çš„ read(n) ä¼šå¤„ç†è§£å‹åçš„æ•°æ®ï¼ˆå¦‚æœ auto_decompress=Trueï¼‰
                        # 256KB å¯¹å…ƒæ•°æ®æŠ“å–ç»°ç»°æœ‰ä½™
                        content_bytes = await response.content.read(262144)
                        html = content_bytes.decode(response.charset or 'utf-8', errors='replace')
                        logger.debug(f"HTML Prefix for {url}: {html[:200]}...")
                    except Exception as e:
                        if "Can not decode content-encoding: br" in str(e):
                             # å¦‚æœæ˜¯ br é”™è¯¯ï¼Œä¸”æˆ‘ä»¬è¿˜æ²¡è£… brotliï¼Œè¿™æ˜¯ä¸€ä¸ªé™çº§ç‚¹
                             logger.debug(f"Brotli ç¼–ç æ— æ³•è§£æï¼Œå°è¯•é™çº§è¯»å–åŸå§‹æµ: {url}")
                             # è¿™ç§æƒ…å†µä¸‹ç›´æ¥è¯»äºŒè¿›åˆ¶æµï¼ˆæœªè§£å‹çš„ï¼‰å°è¯•æ­£åˆ™åŒ¹é…
                             # å› ä¸ºå…ƒæ•°æ®é€šå¸¸æ˜¯ ASCII/UTF-8 å­—ç¬¦ï¼Œåœ¨å‹ç¼©æµä¸­ä¹Ÿæœ‰ä¸€å®šæ¦‚ç‡è¢«æ­£åˆ™åŒ¹é…åˆ°ï¼ˆå¦‚æœæ˜¯ identityï¼‰
                        return None, final_url
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    title = soup.title.string if soup.title else None
                    
                    if not title or not title.strip():
                        meta_og_title = soup.find("meta", attrs={"property": "og:title"})
                        if meta_og_title:
                            title = meta_og_title.get("content")
                        
                        if not title:
                            meta_tw_title = soup.find("meta", attrs={"property": "twitter:title"}) or soup.find("meta", attrs={"name": "twitter:title"})
                            if meta_tw_title:
                                title = meta_tw_title.get("content")
                    
                    if not title:
                        match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
                        if match:
                            title = match.group(1)
                        else:
                            match_og = re.search(r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\'](.*?)["\']', html, re.IGNORECASE)
                            if match_og:
                                title = match_og.group(1)
                            else:
                                # å¾®ä¿¡ç‰¹æœ‰å˜é‡æå–
                                js_title_match = re.search(r'var msg_title = ["\'](.*?)["\'];', html)
                                if js_title_match:
                                    title = js_title_match.group(1)
                                
                                # æŠ–éŸ³ç‰¹æœ‰é€»è¾‘ï¼šæå– RENDER_DATA ä¸­çš„ desc æˆ– title
                                if not title and ("douyin.com" in final_url or "iesdouyin.com" in final_url):
                                    # å°è¯•åŒ¹é… RENDER_DATA
                                    render_data_match = re.search(r'id="RENDER_DATA"[^>]*>(.*?)</script>', html)
                                    if render_data_match:
                                        try:
                                            import json
                                            from urllib.parse import unquote
                                            raw_json = unquote(render_data_match.group(1))
                                            # åœ¨ RENDER_DATA ä¸­æ‰¾ desc å­—æ®µ
                                            desc_match = re.search(r'["\']desc["\']\s*:\s*["\'](.*?)["\']', raw_json)
                                            if desc_match:
                                                title = desc_match.group(1).encode('utf-8').decode('unicode_escape', errors='ignore')
                                        except:
                                            pass
                                    
                                    # å¤‡é€‰ï¼šæ­£åˆ™åŒ¹é…å¸¸è§çš„è§†é¢‘æè¿°å˜é‡
                                    if not title:
                                        video_desc_match = re.search(r'["\'](?:share_)?title["\']\s*:\s*["\'](.*?)["\']', html)
                                        if video_desc_match:
                                            title = video_desc_match.group(1)

                    if title:
                        # æ¸…æ´—æ ‡é¢˜ä¸­çš„è½¬ä¹‰å­—ç¬¦
                        try:
                            if "\\" in title:
                                title = title.encode('utf-8').decode('unicode_escape', errors='ignore')
                        except:
                            pass
                        
                        title = re.sub(r'\s+', ' ', str(title)).strip()
                        if len(title) > 200: title = title[:197] + "..."
                        return title, final_url
                    
                    return None, final_url

        try:
            # ç¬¬ä¸€è½®æŠ“å–
            title, final_url = await try_fetch(use_proxy=not is_domestic)
            
            # å¦‚æœæ˜¯å›½å†…åŸŸåä¸”æ²¡æ‹¿åˆ°æ ‡é¢˜ï¼Œæˆ–è€…æ‹¿åˆ°äº† 200 ä½†å†…å®¹æ˜¯ç©ºçš„ï¼Œå°è¯•ä»£ç†é‡è¯•
            if is_domestic and not title:
                logger.debug(f"ğŸ”„ å›½å†…åŸŸåé¦–è½®è·å–æ ‡é¢˜ä¸ºç©ºï¼Œå°è¯•ä»£ç†é‡è¯•: {url}")
                title, final_url = await try_fetch(use_proxy=True)

            if title:
                self.cache[url] = (title, final_url)
                return title, final_url
            return title, final_url

        except Exception as e:
            error_msg = str(e) or e.__class__.__name__
            if is_domestic:
                try:
                    logger.debug(f"ğŸ”„ å›½å†…åŸŸåå¼‚å¸¸ ({error_msg})ï¼Œå°è¯•ä»£ç†é‡è¯•: {url}")
                    return await try_fetch(use_proxy=True)
                except Exception as e2:
                    logger.warning(f"æŠ“å–å…ƒæ•°æ®å¤±è´¥(é‡è¯•ä¹Ÿå¤±è´¥) {url}: {e2}")
            else:
                logger.warning(f"æŠ“å–å…ƒæ•°æ®å¤±è´¥ {url}: {error_msg}")
            
        return None, None

metadata_provider = MetadataProvider()
