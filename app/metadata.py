import asyncio
import aiohttp
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from app.logger import logger
from typing import Optional
import re

class MetadataProvider:
    """ç½‘é¡µå…ƒæ•°æ®æŠ“å–å™¨ - é‡ç‚¹å…³æ³¨ç¨³å®šæ€§å’Œæ•ˆç‡"""
    
    def __init__(self):
        self.cache = {} # ç®€å•çš„å†…å­˜ç¼“å­˜: {url: title}
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate"
        }

    def _is_safe_url(self, url: str) -> bool:
        """æ£€æŸ¥ URL æ˜¯å¦å®‰å…¨ï¼Œé˜²æ­¢ SSRF æ”»å‡» (ç¦æ­¢å†…ç½‘/æœ¬åœ°è¯·æ±‚)"""
        try:
            parsed = urlparse(url)
            hostname = parsed.hostname.lower() if parsed.hostname else ""
            
            # ç¦æ­¢ç©ºä¸»æœºåæˆ–å¸¸è§çš„æœ¬åœ°/å†…ç½‘æ¨¡å¼
            if not hostname: return False
            if hostname in ["localhost", "127.0.0.1", "0.0.0.0", "::1"]: return False
            
            # ç¦æ­¢ç§æœ‰ IP æ®µ (KISS åŸåˆ™ï¼šæ­£åˆ™åŒ¹é…å¸¸è§å†…ç½‘æ®µ)
            if re.match(r'^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.)', hostname):
                return False
                
            return True
        except:
            return False

    async def fetch_metadata(self, url: str) -> tuple[Optional[str], Optional[str]]:
        """æŠ“å–ç½‘é¡µå…ƒæ•°æ® (æ ‡é¢˜ + æœ€ç»ˆ URL)ï¼Œå¸¦ç¼“å­˜å’Œä¸¥æ ¼è¶…æ—¶"""
        if not url: return None, None
        if not self._is_safe_url(url):
            logger.warning(f"ğŸ›¡ï¸ æ‹¦æˆªæ½œåœ¨çš„ SSRF è¯·æ±‚: {url}")
            return None, None
            
        if url in self.cache: return self.cache[url]

        # é’ˆå¯¹ç‰¹å®šå¹³å°çš„ Headers è°ƒæ•´
        request_headers = self.headers.copy()
        if "douyin.com" in url or "iesdouyin.com" in url:
            # æŠ–éŸ³é€šå¸¸å¯¹ç§»åŠ¨ç«¯ UA æ›´å‹å¥½ï¼Œä¸”é‡å®šå‘é€»è¾‘æ›´ç›´æ¥
            request_headers["User-Agent"] = "Mozilla/5.0 (Linux; Android 10; K) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Mobile Safari/537.36"

        try:
            # 10ç§’è¶…æ—¶ï¼Œé˜²æ­¢é˜»å¡ä¸»æµç¨‹
            async with aiohttp.ClientSession(headers=request_headers) as session:
                async with session.get(url, timeout=10, allow_redirects=True) as response:
                    if response.status != 200:
                        return None, None
                    
                    final_url = str(response.url)

                    # ä»…è¯»å–å‰ 64KBï¼Œè¶³ä»¥åŒ…å« <title> æ ‡ç­¾ï¼ŒèŠ‚çœæµé‡
                    content = await response.content.read(65536)
                    
                    # å°è¯•çŒœæµ‹ç¼–ç ï¼Œé¿å…ä¹±ç 
                    charset = response.charset or 'utf-8'
                    html = content.decode(charset, errors='replace')
                    
                    soup = BeautifulSoup(html, 'html.parser')
                    title = soup.title.string if soup.title else None
                    
                    if title:
                        title = title.strip().replace('\n', ' ').replace('\r', '')
                        # æˆªæ–­è¿‡é•¿çš„æ ‡é¢˜
                        if len(title) > 200: title = title[:197] + "..."
                        
                        self.cache[url] = (title, final_url)
                        return title, final_url
                    
                    # å³ä½¿æ²¡æœ‰æ ‡é¢˜ï¼Œä¹Ÿè¿”å› final_url
                    return None, final_url

        except Exception as e:
            # è®°å½•è­¦å‘Šä½†ä¸æŠ›é”™ï¼Œç¡®ä¿ç¨³å®šæ€§
            logger.warning(f"æŠ“å–å…ƒæ•°æ®å¤±è´¥ {url}: {e}")
            pass
            
        return None, None

metadata_provider = MetadataProvider()
