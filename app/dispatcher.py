import asyncio
from app.config import Config
from app.client import get_client
from app.parser import parse_message
from app.exporter import ExporterFactory
from app.checkpoint import CheckpointManager
from app.logger import logger
from telethon.errors import FloodWaitError
from telethon import utils

class Dispatcher:
    """åˆ†å‘å™¨æ ¸å¿ƒç±»ï¼šè´Ÿè´£æ¶ˆæ¯æ‹‰å–ã€ä»»åŠ¡åŒ¹é…ä¸å¯¼å‡ºè·¯ç”±"""
    
    def __init__(self):
        self.tasks = Config.TASKS
        self.checkpoint = CheckpointManager()
        self.exporters = {} # {path: ExporterInstance}
        self.fieldnames = ['message_id', 'time', 'sender', 'url', 'content', 'source_group', 'source_id', 'reply_to']

    async def _init_task_exporters(self):
        """åˆå§‹åŒ–æœ¬é¡¹ç›®æ‰€æœ‰ä»»åŠ¡éœ€è¦çš„å¯¼å‡ºå™¨"""
        for task in self.tasks:
            if task.output_path not in self.exporters:
                exp = ExporterFactory.create(
                    task.output_format, 
                    task.output_path, 
                    self.fieldnames
                )
                exp.open(mode='a')
                self.exporters[task.output_path] = exp

    def _match_source(self, entity, task_sources) -> bool:
        """åŒ¹é…æ•°æ®æº ID (æ”¯æŒ PeerID å’Œ RawID)"""
        if "all" in task_sources: return True
        p_id = str(utils.get_peer_id(entity))
        raw_id = str(entity.id)
        return any(str(s) in (p_id, raw_id) for s in task_sources)

    async def _get_active_entities(self, client):
        """æ ¹æ®é…ç½®å‘ç°æ‰€æœ‰éœ€è¦æ‰«æçš„ Telegram å®ä½“"""
        explicit_ids, has_all = set(), False
        for task in self.tasks:
            if "all" in task.sources: has_all = True
            for s in task.sources:
                if isinstance(s, (int, str)) and str(s).lstrip('-').isdigit():
                    explicit_ids.add(int(s))
        
        entities = []
        if has_all:
            async for dialog in client.iter_dialogs():
                if dialog.is_group or dialog.is_channel:
                    entities.append(dialog.entity)
        else:
            for sid in explicit_ids:
                try:
                    entities.append(await client.get_entity(sid))
                except Exception as e:
                    logger.error(f"æ— æ³•è·å–å®ä½“ {sid}: {e}")
        return entities

    async def run_cycle(self, client=None):
        """æ‰§è¡Œä¸€ä¸ªå®Œæ•´çš„åˆ†å‘å‘¨æœŸ"""
        if not self.tasks: return

        opened_client = None
        if not client:
            opened_client = await get_client()
            client = opened_client

        await self._init_task_exporters()
        entities = await self._get_active_entities(client)
        
        for entity in entities:
            await self._process_source(client, entity)

        # å‘¨æœŸç»“æŸï¼Œæ¸…ç†èµ„æº
        for exp in self.exporters.values(): exp.close()
        self.exporters.clear() # ç¡®ä¿ä¸‹æ¬¡ rebuild ä»¥åº”ç”¨é…ç½®å˜æ›´

        if opened_client:
            await opened_client.disconnect()
        logger.info("åˆ†å‘å‘¨æœŸæ‰§è¡Œå®Œæ¯•ã€‚")

    async def _process_source(self, client, entity):
        """å¤„ç†å•ä¸ªæ•°æ®æºçš„æ¶ˆæ¯"""
        try:
            source_id = str(utils.get_peer_id(entity))
            group_title = getattr(entity, 'title', source_id)
            
            # 1. é¢„ç­›é€‰ä¸æ­¤æºç›¸å…³çš„ä»»åŠ¡
            active_tasks = [t for t in self.tasks if self._match_source(entity, t.sources)]
            if not active_tasks: return

            # 2. è·å–è¿›åº¦æ–­ç‚¹
            last_id = self.checkpoint.get(source_id, 0)
            logger.info(f"ğŸ”„ æ‰«æ: {group_title} (ID: {source_id}) è‡ª ID: {last_id}")

            count, new_max_id = 0, last_id
            
            # 3. æ‹‰å–æ¶ˆæ¯å¹¶åˆ†å‘
            async for message in client.iter_messages(entity, min_id=last_id, reverse=True):
                if message.id <= last_id: continue

                msg_data = await parse_message(message)
                msg_data['source_group'], msg_data['source_id'] = group_title, source_id
                
                # è·¯ç”±åˆ°å„ä¸ªåŒ¹é…çš„ä»»åŠ¡
                for task in active_tasks:
                    if self._is_task_match(task, msg_data):
                        exporter = self.exporters.get(task.output_path)
                        if exporter and not exporter.is_duplicate(msg_data.get('url')):
                            exporter.write(msg_data)
                            count += 1
                
                if message.id > new_max_id: new_max_id = message.id
            
            # 4. æ›´æ–°æ–­ç‚¹
            if new_max_id > last_id:
                self.checkpoint.set(source_id, new_max_id)

            if count > 0: logger.info(f"âœ… {group_title}: æˆåŠŸè·¯ç”± {count} æ¡æ¶ˆæ¯")

        except FloodWaitError as e:
            logger.warning(f"è§¦å‘é™æµï¼Œç­‰å¾… {e.seconds} ç§’")
            await asyncio.sleep(e.seconds)
        except Exception as e:
            logger.error(f"å¤„ç†æº {source_id} å‡ºé”™: {e}")

    def _is_task_match(self, task, msg_data) -> bool:
        """æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ¹é…ä»»åŠ¡å…³é”®è¯"""
        if not task.keywords: return True
        content = msg_data.get('content', '').lower()
        return any(kw.lower() in content for kw in task.keywords)
